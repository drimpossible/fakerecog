########################
logging outputs to  ../../logs/test
########################
==> Opts and logger initialized..
==> Train and val loaders initialized..
Loaded pretrained weights for efficientnet-b5
==> Model, optimizer, criterion initialized..
==> Training last layer..
Test: [   0/2543]	Time  5.307 ( 5.307)	Loss 0.7763288617134094 (0.7763288617134094)	Acc@1   0.00 (  0.00)
Test: [ 200/2543]	Time  0.170 ( 0.195)	Loss 0.6272977590560913 (0.6917299159723728)	Acc@1  76.56 ( 55.19)
Test: [ 400/2543]	Time  0.171 ( 0.183)	Loss 0.6043052673339844 (0.6837465807684044)	Acc@1 100.00 ( 56.39)
Test: [ 600/2543]	Time  0.171 ( 0.179)	Loss 0.7082902193069458 (0.6878939295767151)	Acc@1  29.69 ( 51.80)
Test: [ 800/2543]	Time  0.171 ( 0.177)	Loss 0.7531003355979919 (0.6919029732321978)	Acc@1   0.00 ( 48.01)
Test: [1000/2543]	Time  0.171 ( 0.176)	Loss 0.707999587059021 (0.692500938247372)	Acc@1  23.44 ( 46.98)
Test: [1200/2543]	Time  0.171 ( 0.175)	Loss 0.6453662514686584 (0.6910741125217187)	Acc@1  76.56 ( 47.57)
Test: [1400/2543]	Time  0.171 ( 0.175)	Loss 0.7209608554840088 (0.691660857770717)	Acc@1  10.94 ( 46.91)
Test: [1600/2543]	Time  0.171 ( 0.174)	Loss 0.7418903708457947 (0.6924306316944602)	Acc@1   9.38 ( 46.26)
Test: [1800/2543]	Time  0.171 ( 0.174)	Loss 0.5899818539619446 (0.6928752932132846)	Acc@1 100.00 ( 45.93)
Test: [2000/2543]	Time  0.171 ( 0.174)	Loss 0.6032354831695557 (0.6919162232002457)	Acc@1 100.00 ( 46.49)
Test: [2200/2543]	Time  0.171 ( 0.174)	Loss 0.7285910844802856 (0.6930194298336041)	Acc@1  29.69 ( 45.52)
Test: [2400/2543]	Time  0.171 ( 0.173)	Loss 0.6029894351959229 (0.692698739112193)	Acc@1 100.00 ( 45.70)
 * Acc@1 45.573 
 * Loss@1 0.6928 
==> Starting pass number: 1, Learning rate: 0.0002
Epoch: [1][     0/244926]	Time  8.348 ( 8.348)	Data  5.975 ( 5.975)	Loss 6.9536e-01 (6.9536e-01)	Acc@1  53.12 ( 53.12)
Epoch: [1][   200/244926]	Time  0.699 ( 0.731)	Data  0.000 ( 0.030)	Loss 4.2791e-01 (6.2126e-01)	Acc@1  81.25 ( 70.50)
Epoch: [1][   400/244926]	Time  0.705 ( 0.714)	Data  0.000 ( 0.015)	Loss 5.3733e-01 (5.7795e-01)	Acc@1  81.25 ( 73.40)
Epoch: [1][   600/244926]	Time  0.696 ( 0.708)	Data  0.000 ( 0.010)	Loss 3.3422e-01 (5.4414e-01)	Acc@1  82.81 ( 75.51)
Test: [   0/2543]	Time  4.875 ( 4.875)	Loss 1.1272612810134888 (1.1272612810134888)	Acc@1  34.38 ( 34.38)
Test: [ 200/2543]	Time  0.171 ( 0.195)	Loss 0.18330660462379456 (1.2899687498372363)	Acc@1  93.75 ( 41.53)
Test: [ 400/2543]	Time  0.171 ( 0.183)	Loss 0.04610978439450264 (0.7435543956223438)	Acc@1 100.00 ( 66.68)
Test: [ 600/2543]	Time  0.171 ( 0.179)	Loss 0.012052996084094048 (0.5607252028583241)	Acc@1 100.00 ( 75.45)
Test: [ 800/2543]	Time  0.171 ( 0.177)	Loss 0.38808074593544006 (0.47896155995024964)	Acc@1  96.88 ( 79.13)
Test: [1000/2543]	Time  0.171 ( 0.176)	Loss 0.6595821976661682 (0.42841391840467713)	Acc@1  62.50 ( 81.35)
Test: [1200/2543]	Time  0.173 ( 0.175)	Loss 0.05819489061832428 (0.393352425542833)	Acc@1 100.00 ( 83.08)
Test: [1400/2543]	Time  0.171 ( 0.175)	Loss 0.5824039578437805 (0.36956072619984554)	Acc@1  56.25 ( 84.14)
Test: [1600/2543]	Time  0.171 ( 0.174)	Loss 0.01706363819539547 (0.3486315368604599)	Acc@1 100.00 ( 85.17)
Test: [1800/2543]	Time  0.171 ( 0.174)	Loss 0.05934763699769974 (0.3287410934240832)	Acc@1 100.00 ( 86.17)
Test: [2000/2543]	Time  0.171 ( 0.174)	Loss 0.2009686976671219 (0.31707700091993146)	Acc@1  98.44 ( 86.64)
Test: [2200/2543]	Time  0.171 ( 0.174)	Loss 0.26630425453186035 (0.3105517292915903)	Acc@1  84.38 ( 86.81)
Test: [2400/2543]	Time  0.171 ( 0.173)	Loss 0.01139503251761198 (0.3016414903933904)	Acc@1 100.00 ( 87.10)
 * Acc@1 87.258 
 * Loss@1 0.2974 
